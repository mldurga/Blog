[
  {
    "objectID": "posts/2021-08-25-ImagesDownload.html",
    "href": "posts/2021-08-25-ImagesDownload.html",
    "title": "Blog",
    "section": "",
    "text": "Following are the ways I used to download images from internet for data analysis purpose * Using search_image_ddg * Using Pyimage search method"
  },
  {
    "objectID": "posts/2021-08-25-ImagesDownload.html#search_image_ddg-method",
    "href": "posts/2021-08-25-ImagesDownload.html#search_image_ddg-method",
    "title": "Blog",
    "section": "search_image_ddg method",
    "text": "search_image_ddg method\nFrom fastbook webiste you can see the documentaion, however here I reproduce code snippet for ease of use.\n\n!pip install fastbook -Uqq\nimport fastbook\nfastbook.setup_book()\n\nMounted at /content/gdrive\n\n\n\nfrom fastbook import *\n\n\ndef search_images_ddg(term, max_images=200): # search_image_ddg in fastbook version has some issues so defining seperately\n    \"Search for `term` with DuckDuckGo and return a unique urls of about `max_images` images\"\n    assert max_images<1000\n    url = 'https://duckduckgo.com/'\n    res = urlread(url,data={'q':term})\n    searchObj = re.search(r'vqd=([\\d-]+)\\&', res)\n    assert searchObj\n    requestUrl = url + 'i.js'\n    params = dict(l='us-en', o='json', q=term, vqd=searchObj.group(1), f=',,,', p='1', v7exp='a')\n    urls,data = set(),{'next':1}\n    while len(urls)<max_images and 'next' in data:\n        try:\n            data = urljson(requestUrl,data=params)\n            urls.update(L(data['results']).itemgot('image'))\n            requestUrl = url + data['next']\n        except (URLError,HTTPError): pass\n        time.sleep(0.2)\n    return L(urls)\n\n\nresults = search_images_ddg('search word', max_images=300)\n\n\ndownload_images(dest, urls=results) #destination folder path"
  },
  {
    "objectID": "posts/2021-08-25-ImagesDownload.html#another-more-elegant-method-using-jmd_imagescraper-thanks-to-joe-dockrill",
    "href": "posts/2021-08-25-ImagesDownload.html#another-more-elegant-method-using-jmd_imagescraper-thanks-to-joe-dockrill",
    "title": "Blog",
    "section": "Another more elegant method using jmd_imagescraper (Thanks to Joe Dockrill)",
    "text": "Another more elegant method using jmd_imagescraper (Thanks to Joe Dockrill)\nRecently I found jmd_imagescraper library from Joe Dockrill. It is lot easier to get images data from web using DuckDuckGo search engine, better than previous method shown.\nFollwoing is the code snippet for downloading any images:\n\n!pip install jmd_imagescraper -Uqq\nfrom jmd_imagescraper.core import *\nfrom pathlib import Path\n\nroot = Path().cwd()/'folder_name'\n\nsearch = duckduckgo_search\n\nsearch(root, 'prez-biden','joe biden ', max_results = 50, img_layout=ImgLayout.All)\n\n\nfrom jmd_imagescraper.imagecleaner import *\ndisplay_image_cleaner(root)\n\n\n\n\n\n\n\n\n\n\nUsing above method we can clean our dataset by deleting whichever is not according to the description of the dataset. using this library we can clearly structure the dataset with proper folder structure."
  },
  {
    "objectID": "posts/2021-08-25-ImagesDownload.html#using-pyimage-search-method",
    "href": "posts/2021-08-25-ImagesDownload.html#using-pyimage-search-method",
    "title": "Blog",
    "section": "Using pyimage search method",
    "text": "Using pyimage search method\nPyimage search is well known for computer vision tutorials and Mr Adrian Rosebrock has given code snippet in his blog here\nYou only need to copy the following code snippet into the Javascript console and type enter. Text file will be downloaded comprising all urls related to the searched images. javascript console will be opened in browser by pressing F12 key in either mozilla firefox or chrome.\n\nfunction simulateRightClick( element ) {\n    var event1 = new MouseEvent( 'mousedown', {\n        bubbles: true,\n        cancelable: false,\n        view: window,\n        button: 2,\n        buttons: 2,\n        clientX: element.getBoundingClientRect().x,\n        clientY: element.getBoundingClientRect().y\n    } );\n    element.dispatchEvent( event1 );\n    var event2 = new MouseEvent( 'mouseup', {\n        bubbles: true,\n        cancelable: false,\n        view: window,\n        button: 2,\n        buttons: 0,\n        clientX: element.getBoundingClientRect().x,\n        clientY: element.getBoundingClientRect().y\n    } );\n    element.dispatchEvent( event2 );\n    var event3 = new MouseEvent( 'contextmenu', {\n        bubbles: true,\n        cancelable: false,\n        view: window,\n        button: 2,\n        buttons: 0,\n        clientX: element.getBoundingClientRect().x,\n        clientY: element.getBoundingClientRect().y\n    } );\n    element.dispatchEvent( event3 );\n}\n\nfunction getURLParam( queryString, key ) {\n    var vars = queryString.replace( /^\\?/, '' ).split( '&' );\n    for ( let i = 0; i < vars.length; i++ ) {\n        let pair = vars[ i ].split( '=' );\n        if ( pair[0] == key ) {\n            return pair[1];\n        }\n    }\n    return false;\n}\n\nfunction createDownload( contents ) {\n    var hiddenElement = document.createElement( 'a' );\n    hiddenElement.href = 'data:attachment/text,' + encodeURI( contents );\n    hiddenElement.target = '_blank';\n    hiddenElement.download = 'urls.txt';\n    hiddenElement.click();\n}\n\nfunction grabUrls() {\n    var urls = [];\n    return new Promise( function( resolve, reject ) {\n        var count = document.querySelectorAll(\n            '.isv-r a:first-of-type' ).length,\n            index = 0;\n        Array.prototype.forEach.call( document.querySelectorAll(\n            '.isv-r a:first-of-type' ), function( element ) {\n            // using the right click menu Google will generate the\n            // full-size URL; won't work in Internet Explorer\n            // (http://pyimg.co/byukr)\n            simulateRightClick( element.querySelector( ':scope img' ) );\n            // Wait for it to appear on the <a> element\n            var interval = setInterval( function() {\n                if ( element.href.trim() !== '' ) {\n                    clearInterval( interval );\n                    // extract the full-size version of the image\n                    let googleUrl = element.href.replace( /.*(\\?)/, '$1' ),\n                        fullImageUrl = decodeURIComponent(\n                            getURLParam( googleUrl, 'imgurl' ) );\n                    if ( fullImageUrl !== 'false' ) {\n                        urls.push( fullImageUrl );\n                    }\n                    // sometimes the URL returns a \"false\" string and\n                    // we still want to count those so our Promise\n                    // resolves\n                    index++;\n                    if ( index == ( count - 1 ) ) {\n                        resolve( urls );\n                    }\n                }\n            }, 10 );\n        } );\n    } );\n}\n\ngrabUrls().then( function( urls ) {\n    urls = urls.join( '\\n' );\n    createDownload( urls );\n} );\n\nAfter downloading the urls text file, upload to the jupyter notebook or download the images just as before\n\ndownload_images(dest='./file', urls=Path('/content/urls.txt'))"
  },
  {
    "objectID": "posts/2021-08-25-ImagesDownload.html#storing-the-collected-images",
    "href": "posts/2021-08-25-ImagesDownload.html#storing-the-collected-images",
    "title": "Blog",
    "section": "storing the collected images",
    "text": "storing the collected images\nDownloaded files from the colab can be stored using the following code snippet\n\n# collect the files\nzip_name = 'folder_name.zip'\n!rm -f {zip_name}\n!zip -q -r {zip_name} {root}\n\n# download the files to your local computer\nfrom google.colab import files\nfiles.download(zip_name)\n\n# download the files to Google Drive\nfrom google.colab import drive\nimport shutil\ndestination_folder = 'folder_name'\ndrive.mount('/content/drive/')\nfolder = Path('/content/drive/My Drive')/destination_folder\nfolder.mkdir(parents=True, exist_ok = True)\nshutil.copyfile(zip_name, str(folder/zip_name))\n\n# open the collected files\n!unzip \\*zip && rm *.zip\n\n\ncredits\n\nMaria L Rodriguez\njoeDockrill\npyimagesearch"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/2021-09-20-Fastbook-chapter13.html",
    "href": "posts/2021-09-20-Fastbook-chapter13.html",
    "title": "Blog",
    "section": "",
    "text": "badges: true branch: master categories: - fastbook date: ‘2021-09-20’ description: Visualising CNN hide: false image: https://user-images.githubusercontent.com/19243618/135568440-9d2fcd92-2538-45db-a4c6-2da0b71327f4.png metadata_key1: metadata_value1 metadata_key2: metadata_value2 output-file: 2021-09-20-fastbook-chapter13.html search: false title: Convolutional Neural Networks. Fastbook chapter 13 toc: true"
  },
  {
    "objectID": "posts/2021-09-20-Fastbook-chapter13.html#inroduction",
    "href": "posts/2021-09-20-Fastbook-chapter13.html#inroduction",
    "title": "Blog",
    "section": "Inroduction",
    "text": "Inroduction\nCNN - Convolutional Neural Networks, is most famous technique used in computer vision applications off-late. Due to rapid innovation and research in this field, these techniques undergo huge transformations in short period of time. Here is my effort to deliberate upon certain terminology and concepts so that it would not be too scary for beginners like me. This topic is a very basic introduction to the concepts and will experiment with CNN and train a classifier of handwritten vernacular vowels of my mother-tongue Telugu.\n\n# from IPython.display import HTML\n# HTML('<img src=\"https://user-images.githubusercontent.com/19243618/135569856-34ff12b8-bcbf-4d16-b500-37fe10f2252e.png\" width=\"400\"/>')\n\nNeural Networks: The typical CNN at high level can be seen as follows\n\n\n\nimage"
  },
  {
    "objectID": "posts/2021-09-20-Fastbook-chapter13.html#terminology",
    "href": "posts/2021-09-20-Fastbook-chapter13.html#terminology",
    "title": "Blog",
    "section": "Terminology",
    "text": "Terminology\n\nNeural Networks:\nNeural networks operates similar to human brain function hence the name. In the context of computer vision, Neural Networks are fed with image type data and output will be any of the deep learning applications like image recognition, object detection or image classification etc. As shown in image, multiple layers of network will get trained to do specified task (ex:image recognition) by applying algorithms and improve accuracy of given task by reducing error.\nHere I am trying to elobarate things through images to help in visualising actual process.\n\nGeneral neural network structure\n\n\nTypical Convolutional Neural Network internals\n\nAbove two visualisations may help in visualising the process of transformation of image data in pixels to useful output\nFollowing are the general terminology used day in and day out to get to know CNN better. Material from the different blogs were collected inaddition to Fastbook for better clarity and visualisation.\n\n\n\nConvolutions, Layers, Filters, Kernels, channels and feature maps\n\nConvolutions\nConvolutions in deeplearning is just multplication and addition of elements in the matices. Following animation will give intuition on single channel convolution on 5X5 matrix with kernel size 3X3 matrix. The purpose of doing convolutions is to extract features from input images by applying various filters. Features may be horizontal edges, vertical edges, circles or diagnoal edges in image. Weights in the filters are automatically learned during training process in CNN.\n\n\n\nLayers and Filters level\nNeural networks comprises number of layers and that number depends on the architecture we choose and the experimentation of the practitioner. Neurons in the first layer are just numbers representing RGB (Red, green, blue) values of color image in case of CNN, and in later layers numbers represents more complex features. Entire CNN is just magic with these numbers - doing multiplicaions and additions. Neurons in the final layer are numbers indicating decision of the network. If network is used to classify between modi with beard and without beard, final layer two neurons can have values either 0 or 1, indicating one is false other is true. Simply, this is all what is happening under the hood.\n\n\nChannels and kernels level\nAny color image feeding into CNN have 3 input channels namely Red, Green, Blue as all the pixel values are RGB values. These are also called as features or feature maps. By applying convolution both no of channels and spatial dimention of incoming feature maps can be transformed. Example: A color modi image has 3 feature maps or channels as shown later.\nKernel is a little matrix which will get multiplied with RGB values in first layer and summed to give an output numbers in output channels. Dont get disheartened by seing lot of numbers, below I have tried to elobarate the concept with visualisations.\n  \nVisualising kernel movements in 3D:\n\nVisualising kernel movements across three channels in 3D:\n\n\n\nStrides and Padding\nAbove animations can give intuition about strides and padding. The kernel size (little matrix), stride and padding will determine the output size after convolution. stride is rate at which kernel moves across the image. It can be 1 pixel or 2 pixels. padding allows us to apply kernel in the image corners. zero padding means adding zero valued pixels on each side to accomadate the kernel movement completely on all sides. Please read more here for convolution arithmetic.\nTo elobarate the above concepts take an example color image of modi\n\nimg='https://user-images.githubusercontent.com/19243618/134114426-d093484d-ae13-4025-8c43-c9dcbffa4d84.png'\nurllib.request.urlretrieve(img,'modi')\nim=Image.open('modi')\nshow_image(im)\n\n<matplotlib.axes._subplots.AxesSubplot>\n\n\n\n\n\nThis image has three channels namely red, green, blue and will be stacked and fed as 3D block to CNN. These channels can be seen from the images below.\n\nim = image2tensor(Image.open('modi'))\nim.shape\n_,axs = subplots(1,3)\nfor modi,ax,color in zip(im,axs,('Reds','Greens','Blues')):\n    show_image(255-modi, ax=ax, cmap=color)\n\n\n\n\nabove three channels of the image can be stacked as follows\n \nWhen a kernel is applied on to these channels new feature or output channel will be created. Under the hood, the numbers from three incoming channels will be multiplied by corresponding weight values in kernel and summed to give final number that will be part of the output channel. This can be visualised as follows.  \nThis is what is happening at the kernel level. At whole image level, following 3D image gives better intuition.\n \nThough it looks as 3D convolution, actually it is 2D convolution. First block (blue) here can be 3 channels of modi images, or any layer with certain number of incoming channels (64, 128, 256 etc…). By matching kernel numbers stacked together in the filter (orange block), with incoming channels, its effectively 2D convolution. Every kernel has weights (again numbers) and get multiplied with channels and summedup to give one output channel per filter, that is what final block (green) came out here. For a real world example consider the following image from CS231n website.\n \nso the car image got CNN with 3 incoming channels(blue) and 10 filters (orange) in first layer to give 10 feature maps or channels (green) as output. Now to visualise number of filters and number of output channels from given input channels please look at following.\n \nImagine there are 10 filters (orange blocks) getting multiplied and added with input channels (blue block) will gives rise to 10 output channels. This new block will become input to the next layer with 10 incoming channels. So number of output channels depends on number of filters. This is how the features will get transformed and forwarded to next layers until it reaches specified task. Follwoing image will give intuition about feature tranformations taking place in CNN in various layers.\n \n\n\n\n1X1 Convolutions or Network in Network\nUnlike convolutions mentioned above, here 1X1 kernel size is used to reduce the no of channels. For example, if a particular feature map has 192 features, it can be reduced to single feature map without effecting its feature map size. This is called as cross channel downsampling or dimensionality reduction. In a way, the new feature map stores infromation from previous multiple feature maps. Following visualisation can help in intuition.\n\n\n\nimage\n\n\nThis operation is primarily done to reduce the computational load during convolutions. Example from this helps to understand better. Example: If we want to convolve 28X28X192 input feature maps with 5X5X32 filters, this will result in ~120 million operations.\n\n\n\nimage\n\n\nThe computation will be greatly reduced by introducing 1X1 conv layer before 5X5 conv layer as shown below.\n\n\n\nimage\n\n\nfor further visual understanding of Convolutional neural networks I highly encourage you to look at this video."
  },
  {
    "objectID": "posts/2021-09-20-Fastbook-chapter13.html#applying-cnn-on-vernacular-language-telugu",
    "href": "posts/2021-09-20-Fastbook-chapter13.html#applying-cnn-on-vernacular-language-telugu",
    "title": "Blog",
    "section": "Applying CNN on vernacular language ‘Telugu’",
    "text": "Applying CNN on vernacular language ‘Telugu’\nHere we use fastai code and ready built methods to work with telugu language charecters. However first demonstartion will be shown with MNIST dataset (handwritten numbers) and then proceed with telugu hand written charecters.\n\nWorking with MNIST dataset using fastai\n\npath=untar_data(URLs.MNIST)\ndls=ImageDataLoaders.from_folder(path,train='training',valid='testing',batch_tfms=Normalize())\nlearn = cnn_learner(dls, resnet18, pretrained=False,loss_func=F.cross_entropy, metrics=accuracy, n_out=10)\nlearn.fit_one_cycle(2)\n\n\n    \n        \n      \n      100.03% [15687680/15683414 00:00<00:00]\n    \n    \n\n\n/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.136468\n      0.107577\n      0.965600\n      01:50\n    \n    \n      1\n      0.036308\n      0.024930\n      0.991800\n      01:50\n    \n  \n\n\n\nThe beauty of fastai is state of the art accuracy can be obtained within 5 lines of code. No other Deep learning library has done this thing yet.\nso lets apply the same technique on handwritten telugu vowels of 16 charecters.\n\n\nNow lets play with telugu vowels dataset\nthe dataset is downloaded from IEEEdataport website and the autor of the dataset is Mr. Munisekhar velapuru. You can download dataset after registering in their website.\n\nurl='https://ieee-dataport.s3.amazonaws.com/open/26845/Final%20Dataset%20of%20Telugu%20Handwritten%20Chararcters.zip?response-content-disposition=attachment%3B%20filename%3D%22Final%20Dataset%20of%20Telugu%20Handwritten%20Chararcters.zip%22&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJOHYI4KJCE6Q7MIQ%2F20210930%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210930T192654Z&X-Amz-SignedHeaders=Host&X-Amz-Expires=86400&X-Amz-Signature=d91e04b5fbfd5547c8677ad5946c6d83e3d5c4531b69872a00a5dc72c5a7721b'\n\n\ntelugu=untar_data(url)\n\ntelugu.ls()\n\n\n    \n        \n      \n      100.00% [351764480/351763674 00:05<00:00]\n    \n    \n\n\n(#1) [Path('/root/.fastai/data/Final%20Dataset%20of%20Telugu%20Handwritten%20Chararcters/Test1')]\n\n\nHere we are only taking vowels data, though this dataset consists of whole lot of Telugu vottulu, hallulu, guninthalu.\n\npath_t=Path(telugu/'Test1'/'achulu')\n\nDefining dataloader function\n\ndef get_dls(bs=64):\n  tel_block=DataBlock(blocks=(ImageBlock, CategoryBlock),\n                      get_items=get_image_files,\n                      get_y=parent_label,\n                      splitter=RandomSplitter(valid_pct=0.2, seed=42),\n                      batch_tfms=Normalize(),\n                      item_tfms=Resize(256))\n  return tel_block.dataloaders(path_t, bs=bs)\n\nLets look at typical batch of vowels\n\ndls=get_dls()\ndls.show_batch(max_n=9,figsize=(4,4))\n\n\n\n\nhere we are using resnet34 since need to recognise 16 charecters and dataset is little messy as can be seen from show_batch\n\nlearn_te=cnn_learner(dls,resnet34, metrics=accuracy)\n\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n\n\n\n\n\nlearn_te.fine_tune(10)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      2.527853\n      1.379285\n      0.548979\n      02:33\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.021525\n      0.566731\n      0.808277\n      03:32\n    \n    \n      1\n      0.462154\n      0.322305\n      0.896281\n      03:32\n    \n    \n      2\n      0.216439\n      0.258121\n      0.921425\n      03:32\n    \n    \n      3\n      0.114072\n      0.233685\n      0.928759\n      03:32\n    \n    \n      4\n      0.079014\n      0.224550\n      0.939759\n      03:32\n    \n    \n      5\n      0.040476\n      0.115557\n      0.963332\n      03:32\n    \n    \n      6\n      0.021079\n      0.125687\n      0.968046\n      03:32\n    \n    \n      7\n      0.009155\n      0.118855\n      0.969618\n      03:32\n    \n    \n      8\n      0.003386\n      0.106660\n      0.973284\n      03:32\n    \n    \n      9\n      0.002528\n      0.107360\n      0.972761\n      03:32"
  },
  {
    "objectID": "posts/2021-09-20-Fastbook-chapter13.html#credits",
    "href": "posts/2021-09-20-Fastbook-chapter13.html#credits",
    "title": "Blog",
    "section": "Credits",
    "text": "Credits\n\nFastbook chapter 13 is helped me in understanding internals of convolutions\nMany of the images were collected from this blog by kunlun bai, and this by sumit saha\ncs231n is also helped me for this blog\nPlease see this video for indepth visualisation of network"
  },
  {
    "objectID": "posts/2021-08-26-MIT_exercise_2.html",
    "href": "posts/2021-08-26-MIT_exercise_2.html",
    "title": "Blog",
    "section": "",
    "text": "MIT Missing semester Lectures are greatly helps one to work with terminal, git -version control systems, data wrangling etc… Here I have listed out solutions for the exercises of each lecture.\n\nRead man ls and write an ls command that lists files in the following manner Includes all files, including hidden files Sizes are listed in human readable format (e.g. 454M instead of 454279954) Files are ordered by recency Output is colorized\n\n\n!ls -laht --color=always\n\ntotal 28K\ndrwxr-xr-x 1 root root 4.0K Aug 26 06:56 .\ndrwxr-xr-x 2 root root 4.0K Aug 26 06:56 bar\ndrwxr-xr-x 2 root root 4.0K Aug 26 06:56 foo\ndrwxr-xr-x 2 root root 4.0K Aug 26 06:56 fooob\ndrwxr-xr-x 1 root root 4.0K Aug 26 06:41 ..\ndrwxr-xr-x 1 root root 4.0K Aug 13 13:35 sample_data\ndrwxr-xr-x 4 root root 4.0K Aug 13 13:34 .config\n\n\n\nWrite bash functions marco and polo that do the following. Whenever you execute marco the current working directory should be saved in some manner, then when you execute polo, no matter what directory you are in, polo should cd you back to the directory where you executed marco. For ease of debugging you can write the code in a file marco.sh and (re)load the definitions to your shell by executing source marco.sh.\n   touch marco.sh polo.sh\n   chmod +x marco.sh polo.sh          \n\n   marco.sh\n   marco () {\n         pwd > marco.txt\n   }\n\n   polo.sh\n   polo () {\n         cd $(cat < marco.txt)\n   }\n\nafter creating marco.sh and polo.sh files in vim call them from anywhere using source command\n        source path/to/workingdir/marco.sh\n\n        source path/to/workingdir/polo.sh\n\nSay you have a command that fails rarely. In order to debug it you need to capture its output but it can be time consuming to get a failure run. Write a bash script that runs the following script until it fails and captures its standard output and error streams to files and prints everything at the end. Bonus points if you can also report how many runs it took for the script to fail."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Weblogs",
    "section": "",
    "text": "I am deeply passionate about deep learning. I have started my journey into ML/AI space in 2020 and have been overwhelmed and fascinated by its sheer curiosity generating concepts. This blog is a way to store and publish my understandings and if it helps anybody in anyway that would be great. \n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nFastai fit_one_cycle & fine_tune and Super-Convergence (Leslie Smith)\n\n\n\n\n\n\n\npaper_reading\n\n\n\n\nExploring Source code of fastai\n\n\n\n\n\n\nOct 14, 2021\n\n\n\n\n\n\n  \n\n\n\n\nConvolutional Neural Networks. Fastbook chapter 13\n\n\n\n\n\n\n\nfastbook\n\n\n\n\nVisualising CNN\n\n\n\n\n\n\nSep 20, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMissing semester MIT Exercise 2 Solutions\n\n\n\n\n\n\n\n\n\n\n\n\nAug 26, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImages downloading methods\n\n\n\n\n\n\n\n\n\n\n\n\nAug 25, 2021\n\n\n\n\n\n\n  \n\n\n\n\nModi’s Beard. Fastai chapter 2\n\n\n\n\n\n\n\nfastbook\n\n\n\n\nWant to play with Modi’s beard…\n\n\n\n\n\n\nAug 4, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/2021-08-04-Fastbook-chapter2.html",
    "href": "posts/2021-08-04-Fastbook-chapter2.html",
    "title": "Blog",
    "section": "",
    "text": "badges: true branch: master categories: - fastbook date: ‘2021-08-04’ description: “Want to play with Modi’s beard…” hide: false image: https://user-images.githubusercontent.com/19243618/137372923-f83f6800-9bac-43e8-94c3-dbf5c82acfd4.png metadata_key1: metadata_value1 metadata_key2: metadata_value2 output-file: 2021-08-04-fastbook-chapter2.html search: false"
  },
  {
    "objectID": "posts/2021-08-04-Fastbook-chapter2.html#play-with-beard-first",
    "href": "posts/2021-08-04-Fastbook-chapter2.html#play-with-beard-first",
    "title": "Blog",
    "section": "Play with Beard First !",
    "text": "Play with Beard First !\nIt is very exciting for me to share with you the recent experiments with deep learning after introduced to fast.ai. Just open this link and upload any modi (Indian PM) photo either from 2014 or 2021 (you know difference is beard :smiley:), my model will predict whether its from 2014 or 2021 with nearly 93% accuracy. Journey into deep learning is fun and exhilarating. I seriously urge you to look at fast.ai if you are interested in deep learning. Now, lets dive into my blog."
  },
  {
    "objectID": "posts/2021-08-04-Fastbook-chapter2.html#what-is-fastai",
    "href": "posts/2021-08-04-Fastbook-chapter2.html#what-is-fastai",
    "title": "Blog",
    "section": "What is Fastai",
    "text": "What is Fastai\nFastai is front runner and research organisation working on making Deep Learning as democratic as possible with beautiful community support. Fastai in a way cutshorts the path needed for one to become a great ML practitioner and honestly stands to the title of its popular book: Deep Learning for coders with fastai & PyTorch (AI Applicaitons without a PhD). I sincearly want to thank Mr Jeremy Howard and Mr Sylvain Gugger for presenting such an awesome gem in just 600 pages. The series of posts that I am going to write on this book is in a way to solidfy my understanding of the concepts taught in the book and practice writing as encouraged greatly by authors and community. I hope this turns out to be good source to refer for me and beginners who are just behind me."
  },
  {
    "objectID": "posts/2021-08-04-Fastbook-chapter2.html#what-tools-do-you-need",
    "href": "posts/2021-08-04-Fastbook-chapter2.html#what-tools-do-you-need",
    "title": "Blog",
    "section": "What tools do you need",
    "text": "What tools do you need\nMind you we are just beginners and we dont need anything more than Google colab scratchpad. This will spinup notebook with free GPU (Graphical Processing Unit) for you with just click of a mouse. Great colab tips can be found here. Full server notebooks like GCP and AWS instances are still difficult to setup for complete beginners like me. Believe me, I have spent one whole month in understanding how to setup GCP instance. Now almost all the time I spinup colab for any course work for its ease and simplicity. People like Zachery Mueller, who is like poster child for fastai community, prefers to practice with colab."
  },
  {
    "objectID": "posts/2021-08-04-Fastbook-chapter2.html#domains-of-deep-learning",
    "href": "posts/2021-08-04-Fastbook-chapter2.html#domains-of-deep-learning",
    "title": "Blog",
    "section": "Domains of Deep Learning",
    "text": "Domains of Deep Learning\nDeep learning can be applied to wide variety of tasks and tinkering mindset can force one to enable applying it in creative ways. The increasing availabilty of data in any field will provide possible use case and can leverage new insights. However, the current research and practice broadly falls under following categories.\n\nComputer Vision:\nWhat do you think Tesla FSD (Full Self Driving) cars equipped with for its Autopilot system? Computer vision plays central bedrock role for autonomous driving. Today computers are as good as people in identifying objects in an image using their neural networks. We take up computer vision in this chapter and slowly expand to other domains as course progress. Any deep learning system predictions depend on the quality of the training data that we provide. Certainly this book provides tools and strategies to get state of the art accuracy with innovative methods even with limited data.\n\n\nNatural Language Processing NLP:\nDid you ever hear GPT-3, GPT-2, Transformers, Huggingface, twitter bots, google translate etc. in deep learning space, then all these terms belong to text domain of deep learning. This domain deals with how computers handles text data and its response for a specified task. Today NLP tasks ranges from classifying documents, sentiment analysis (positive, negetive), summarising documents, context appropriate text generation etc. Downside of technology ranges from ‘throwing abuses by bot’ to ‘bot generated twitter trolls at massive scale’. However, Google translate massively used by people in ever increasing multicultural society of today.\n\n\nTabular Data & RecSys:\nMost common ML applications we find at industry is tabular data based. Though traditional ML techniques like random forests and gradient boost techniques still hold good, deep learning offlate making strides into tabular data domain. RecSys - Recommendation System is also type of tabular data but the data is highly cardinal and deep learning can be applied to such data with good results. E-commerce websites like amazon applies deep learning techniques for its user-table to recommend products."
  },
  {
    "objectID": "posts/2021-08-04-Fastbook-chapter2.html#thinking-approach",
    "href": "posts/2021-08-04-Fastbook-chapter2.html#thinking-approach",
    "title": "Blog",
    "section": "Thinking Approach",
    "text": "Thinking Approach\nWhen solving any problem at hand with ML and deep learning techniques, the fundamental thinking approach should be the predictions of the model should be helpful to the real world. Jeremy Howard named this approach as Drivetrain Approach and his elegant presentation can be seen here. His key ideas are as follows: 1. Objective: What outcome am I trying to achieve 2. Levers: What inputs can we control 3. Data: What data can we collect 4. Models: How the levers influence the objective\nThouogh seemingly simple concept, its application in real world has great impact on the outcome of the modeling. Ingeneral the data we have on hand may not be causal and the outcomes may not represent the actual conditions. If we can control certain input conditions by injecting randomness the causal relationships can be collected and new data will become food for the model and thus accuracy of the predictions can be improved."
  },
  {
    "objectID": "posts/2021-08-04-Fastbook-chapter2.html#lets-dive-in",
    "href": "posts/2021-08-04-Fastbook-chapter2.html#lets-dive-in",
    "title": "Blog",
    "section": "Lets Dive In",
    "text": "Lets Dive In\nTo make your first dive in experience into deep learning little more interesting, I have choosen a “Modi’s Beard” project. Interesting..! yes. You upload any of the present Indian PM “Narendra modi” image, my app will tell whether the image is “2014 Modi” or “2021 Modi”. This we are doing with computer vision deep learning techniques. This notebook will detail each and every step of the process.\n\nGathering Data\nThough “Bing Image Search” was the primary source referred by book, offlate course.fast.ai website given alternative DuckDuckGo as this one dont need any key. Please read docs for more information on this.\n\nfrom fastbook import *\nfrom ipywidgets import *\n\n\n!pip install fastai --upgrade -Uqq\n\n\nimport fastai\nfastai.__version__\n\n'2.5.2'"
  },
  {
    "objectID": "posts/2021-08-04-Fastbook-chapter2.html#working-with-bears-first",
    "href": "posts/2021-08-04-Fastbook-chapter2.html#working-with-bears-first",
    "title": "Blog",
    "section": "Working with Bears First",
    "text": "Working with Bears First\nFirst we will learn how to train our first ML model to recognise “type of bear” images then we will go ahead with “Modi Beard Experimentation”. In a way this ensures I am on right track and not messedup anything.\n\nDownloading Images with DuckDuckGo API\n\nbear_types='grizzly','black','teddy' # types of bears we want to identify\npath=Path('bears') # creating bears folder path for storing all data\n\nif not path.exists(): # bears folder will be created if doesnt exists\n    path.mkdir()\n    for o in bear_types:\n        dest = (path/o) # creating path for each type of bear\n        dest.mkdir(exist_ok=True) # creating child folders under bears\n        results = search_images_ddg(f'{o} bear', max_images=150) # Collecting URLs from search engine\n        download_images(dest, urls=results) # downloading URLs at specified folders\n\n\n\nChecking for failed images\nMany times images from the URLs might not be working and we wanted to remomve such images from our training set. Fastai provides good function to remove and unlink the path to those images in our data.\n\nfns=get_image_files(path) # collecting all downloded image files paths (fns - filenames)\nfns\n\nfailed=verify_images(fns) # verifying wether image is ok or not\nfailed\n\nfailed.map(Path.unlink) # unlinking failed images\n\n(#0) []\n\n\n\n\nCreating DataBlocks and DataLoaders\nThis will ease the whole process of data handling and channelise the data into model and facilitates both training and validation sets for model. Please explore these videos by Vsihnu Subramaniyam for further undestanding.\n\n \nbears = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), # This will tell what kind of data we are working with\n    get_items=get_image_files, # it will collect all image files from the path\n    splitter=RandomSplitter(valid_pct=0.2, seed=42), # splits data into train and valid sets\n    get_y=parent_label, # labelling of data for each image\n    item_tfms=RandomResizedCrop(224,0.5), batch_tfms=aug_transforms()) # Resizing of each image to standard size and augmentaton of image\n\n#now the above template will be fed with actual data source, here it is 'path'\ndls=bears.dataloaders(path) # data in 'path' will be fed to DataBlock through dataloders by filepaths\n\ndls.show_batch() in a way helps us to check wether everything was went well or not. This will show us a batch of images that are going under hammer for training.\n\ndls.show_batch()\n\n\n\n\n\n\nTraining Bears Model\nResNet18 is a model already trained on ImageNet, and can act like backbone for State of The Art performance on image recognition. we are using resnet18 for this training task using convolutional neural netowrks. Pretrained model is already good at recognising many types of images and here that model will be nudged or finetuned to work on our task of recognising bears.\n\nlearn=cnn_learner(dls,resnet18, metrics=error_rate)\nlearn.fine_tune(4,7e-4)\n\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.555692\n      0.435306\n      0.229358\n      00:33\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.484510\n      0.172394\n      0.064220\n      00:33\n    \n    \n      1\n      0.361524\n      0.090742\n      0.036697\n      00:34\n    \n    \n      2\n      0.287017\n      0.073770\n      0.027523\n      00:33\n    \n    \n      3\n      0.232049\n      0.072706\n      0.027523\n      00:33\n    \n  \n\n\n\ntraining seems quite good as validation error rate has decreased to 2.8% i.e accuracy is 97.2%. we can see the confusion matrix and can analyse where exactly model is confusing between different bears. Top losses will show difference between prediction and actual image and also gives the confidence with which it is predicting.\n\ninterp=ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(5)"
  },
  {
    "objectID": "posts/2021-08-04-Fastbook-chapter2.html#lets-play-with-beard-now",
    "href": "posts/2021-08-04-Fastbook-chapter2.html#lets-play-with-beard-now",
    "title": "Blog",
    "section": "Lets Play With Beard Now",
    "text": "Lets Play With Beard Now\nThe above code snippet for image recognition is working at State of The Art accuracy and we can apply same understanding on the recognition of Modi Beard, wether is it from 2014 or 2021? For collecting image data we can use same duckduckgo search engine.\n\n!pip install gdown -qq\n!gdown https://drive.google.com/uc?id=1D3OmFPEKba4b24pfJYcoWRh9ITr-gOMP -O tmp.zip\n!unzip -q tmp.zip -d ./tmp \n!rm tmp.zip\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1D3OmFPEKba4b24pfJYcoWRh9ITr-gOMP\nTo: /content/tmp.zip\n46.7MB [00:00, 128MB/s]\n\n\n\npath2=Path('./tmp')\nfns=get_image_files(path2)\nfns\n\n(#315) [Path('tmp/modi/2021/00000179.jpg'),Path('tmp/modi/2021/00000239.jpg'),Path('tmp/modi/2021/00000044.jpg'),Path('tmp/modi/2021/00000132.png'),Path('tmp/modi/2021/00000244.jpg'),Path('tmp/modi/2021/00000059.jpg'),Path('tmp/modi/2021/00000165.jpg'),Path('tmp/modi/2021/00000206.jpg'),Path('tmp/modi/2021/00000075.jpg'),Path('tmp/modi/2021/00000112.jpg')...]\n\n\n\nfailed2=verify_images(fns)\nfailed2.map(Path.unlink)\n\n(#0) []\n\n\n\nmodi = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=RandomResizedCrop(224,0.5), batch_tfms=aug_transforms())\n\ndls2=modi.dataloaders(path2)\n\n\ndls2.show_batch()\n\n\n\n\n\nlearn2=cnn_learner(dls2,resnet18,metrics=error_rate)\n# learn2.lr_find()\n\n\nlearn2.fine_tune(10,0.0017)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.136465\n      0.812668\n      0.412698\n      00:07\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.919497\n      0.840486\n      0.317460\n      00:08\n    \n    \n      1\n      0.816630\n      0.719514\n      0.285714\n      00:08\n    \n    \n      2\n      0.730438\n      0.461705\n      0.190476\n      00:08\n    \n    \n      3\n      0.606138\n      0.337663\n      0.126984\n      00:08\n    \n    \n      4\n      0.536673\n      0.297072\n      0.126984\n      00:08\n    \n    \n      5\n      0.464356\n      0.289846\n      0.126984\n      00:08\n    \n    \n      6\n      0.421824\n      0.260043\n      0.111111\n      00:08\n    \n    \n      7\n      0.381456\n      0.214176\n      0.079365\n      00:08\n    \n    \n      8\n      0.364465\n      0.201804\n      0.063492\n      00:07\n    \n    \n      9\n      0.340084\n      0.192317\n      0.063492\n      00:08\n    \n  \n\n\n\nAs you can see we could achieve nearly 93% accuracy, but this further can be improved by investigating plot losses function of fastai.\n\ninterp=ClassificationInterpretation.from_learner(learn2)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(5)\n\n\n\n\nyou can very well see that some of the images were no where related to modi, but these are the prime reasons for decreasing efficiency in prediction. Accuracy further can be increased by removing all that data from the files. Now lets build an app to recognise a beard modi from 2014 or 2021."
  },
  {
    "objectID": "posts/2021-08-04-Fastbook-chapter2.html#building-app",
    "href": "posts/2021-08-04-Fastbook-chapter2.html#building-app",
    "title": "Blog",
    "section": "Building app",
    "text": "Building app\nwe can build and deploy an application on the go and anybody can test the app by uploading an image of modi from their phones or internet, and app will predict wethere the image is from 2014 or 2021.\n\n!gdown https://drive.google.com/uc?id=1cu7wEQP99Y5F_0LJ8BtuC1T0uzBLH_kt\nlearn_inf=load_learner('export.pkl')\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1cu7wEQP99Y5F_0LJ8BtuC1T0uzBLH_kt\nTo: /content/export.pkl\n47.0MB [00:00, 129MB/s] \n\n\n\nlearn_inf.dls.vocab\n\n['2014', '2021']\n\n\n\nbtn_upload = widgets.FileUpload()\nbtn_upload\n\n\nimg=PILImage.create(btn_upload.data[-1])\nout_pl = widgets.Output()\nout_pl.clear_output()\nwith out_pl: display(img.to_thumb(128,128))\nout_pl\n\n\npred,pred_idx,probs = learn_inf.predict(img)\n\n\n\n\n\nlbl_pred = widgets.Label()\nlbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\nlbl_pred\n\n\n\n\n\nbtn_run = widgets.Button(description='Classify')\nbtn_run\ndef on_click_classify(change):\n    img = PILImage.create(btn_upload.data[-1])\n    out_pl.clear_output()\n    with out_pl: display(img.to_thumb(128,128))\n    pred,pred_idx,probs = learn_inf.predict(img)\n    lbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\n\nbtn_run.on_click(on_click_classify)\n\n\nbtn_upload = widgets.FileUpload()\n\n\nVBox([widgets.Label('Select your bear!'), \n      btn_upload, btn_run, out_pl, lbl_pred])"
  },
  {
    "objectID": "posts/2021-08-04-Fastbook-chapter2.html#toc-true",
    "href": "posts/2021-08-04-Fastbook-chapter2.html#toc-true",
    "title": "Blog",
    "section": "toc: true",
    "text": "toc: true"
  },
  {
    "objectID": "posts/2021-08-25-imagesdownload.html",
    "href": "posts/2021-08-25-imagesdownload.html",
    "title": "Images downloading methods",
    "section": "",
    "text": "Following are the ways I used to download images from internet for data analysis purpose * Using search_image_ddg * Using Pyimage search method"
  },
  {
    "objectID": "posts/2021-08-25-imagesdownload.html#search_image_ddg-method",
    "href": "posts/2021-08-25-imagesdownload.html#search_image_ddg-method",
    "title": "Images downloading methods",
    "section": "search_image_ddg method",
    "text": "search_image_ddg method\nFrom fastbook webiste you can see the documentaion, however here I reproduce code snippet for ease of use.\n\n!pip install fastbook -Uqq\nimport fastbook\nfastbook.setup_book()\n\nMounted at /content/gdrive\n\n\n\nfrom fastbook import *\n\n\ndef search_images_ddg(term, max_images=200): # search_image_ddg in fastbook version has some issues so defining seperately\n    \"Search for `term` with DuckDuckGo and return a unique urls of about `max_images` images\"\n    assert max_images<1000\n    url = 'https://duckduckgo.com/'\n    res = urlread(url,data={'q':term})\n    searchObj = re.search(r'vqd=([\\d-]+)\\&', res)\n    assert searchObj\n    requestUrl = url + 'i.js'\n    params = dict(l='us-en', o='json', q=term, vqd=searchObj.group(1), f=',,,', p='1', v7exp='a')\n    urls,data = set(),{'next':1}\n    while len(urls)<max_images and 'next' in data:\n        try:\n            data = urljson(requestUrl,data=params)\n            urls.update(L(data['results']).itemgot('image'))\n            requestUrl = url + data['next']\n        except (URLError,HTTPError): pass\n        time.sleep(0.2)\n    return L(urls)\n\n\nresults = search_images_ddg('search word', max_images=300)\n\n\ndownload_images(dest, urls=results) #destination folder path"
  },
  {
    "objectID": "posts/2021-08-25-imagesdownload.html#another-more-elegant-method-using-jmd_imagescraper-thanks-to-joe-dockrill",
    "href": "posts/2021-08-25-imagesdownload.html#another-more-elegant-method-using-jmd_imagescraper-thanks-to-joe-dockrill",
    "title": "Images downloading methods",
    "section": "Another more elegant method using jmd_imagescraper (Thanks to Joe Dockrill)",
    "text": "Another more elegant method using jmd_imagescraper (Thanks to Joe Dockrill)\nRecently I found jmd_imagescraper library from Joe Dockrill. It is lot easier to get images data from web using DuckDuckGo search engine, better than previous method shown.\nFollwoing is the code snippet for downloading any images:\n\n!pip install jmd_imagescraper -Uqq\nfrom jmd_imagescraper.core import *\nfrom pathlib import Path\n\nroot = Path().cwd()/'folder_name'\n\nsearch = duckduckgo_search\n\nsearch(root, 'prez-biden','joe biden ', max_results = 50, img_layout=ImgLayout.All)\n\n\nfrom jmd_imagescraper.imagecleaner import *\ndisplay_image_cleaner(root)\n\n\n\n\n\n\n\n\n\n\nUsing above method we can clean our dataset by deleting whichever is not according to the description of the dataset. using this library we can clearly structure the dataset with proper folder structure."
  },
  {
    "objectID": "posts/2021-08-25-imagesdownload.html#using-pyimage-search-method",
    "href": "posts/2021-08-25-imagesdownload.html#using-pyimage-search-method",
    "title": "Images downloading methods",
    "section": "Using pyimage search method",
    "text": "Using pyimage search method\nPyimage search is well known for computer vision tutorials and Mr Adrian Rosebrock has given code snippet in his blog here\nYou only need to copy the following code snippet into the Javascript console and type enter. Text file will be downloaded comprising all urls related to the searched images. javascript console will be opened in browser by pressing F12 key in either mozilla firefox or chrome.\n\nfunction simulateRightClick( element ) {\n    var event1 = new MouseEvent( 'mousedown', {\n        bubbles: true,\n        cancelable: false,\n        view: window,\n        button: 2,\n        buttons: 2,\n        clientX: element.getBoundingClientRect().x,\n        clientY: element.getBoundingClientRect().y\n    } );\n    element.dispatchEvent( event1 );\n    var event2 = new MouseEvent( 'mouseup', {\n        bubbles: true,\n        cancelable: false,\n        view: window,\n        button: 2,\n        buttons: 0,\n        clientX: element.getBoundingClientRect().x,\n        clientY: element.getBoundingClientRect().y\n    } );\n    element.dispatchEvent( event2 );\n    var event3 = new MouseEvent( 'contextmenu', {\n        bubbles: true,\n        cancelable: false,\n        view: window,\n        button: 2,\n        buttons: 0,\n        clientX: element.getBoundingClientRect().x,\n        clientY: element.getBoundingClientRect().y\n    } );\n    element.dispatchEvent( event3 );\n}\n\nfunction getURLParam( queryString, key ) {\n    var vars = queryString.replace( /^\\?/, '' ).split( '&' );\n    for ( let i = 0; i < vars.length; i++ ) {\n        let pair = vars[ i ].split( '=' );\n        if ( pair[0] == key ) {\n            return pair[1];\n        }\n    }\n    return false;\n}\n\nfunction createDownload( contents ) {\n    var hiddenElement = document.createElement( 'a' );\n    hiddenElement.href = 'data:attachment/text,' + encodeURI( contents );\n    hiddenElement.target = '_blank';\n    hiddenElement.download = 'urls.txt';\n    hiddenElement.click();\n}\n\nfunction grabUrls() {\n    var urls = [];\n    return new Promise( function( resolve, reject ) {\n        var count = document.querySelectorAll(\n            '.isv-r a:first-of-type' ).length,\n            index = 0;\n        Array.prototype.forEach.call( document.querySelectorAll(\n            '.isv-r a:first-of-type' ), function( element ) {\n            // using the right click menu Google will generate the\n            // full-size URL; won't work in Internet Explorer\n            // (http://pyimg.co/byukr)\n            simulateRightClick( element.querySelector( ':scope img' ) );\n            // Wait for it to appear on the <a> element\n            var interval = setInterval( function() {\n                if ( element.href.trim() !== '' ) {\n                    clearInterval( interval );\n                    // extract the full-size version of the image\n                    let googleUrl = element.href.replace( /.*(\\?)/, '$1' ),\n                        fullImageUrl = decodeURIComponent(\n                            getURLParam( googleUrl, 'imgurl' ) );\n                    if ( fullImageUrl !== 'false' ) {\n                        urls.push( fullImageUrl );\n                    }\n                    // sometimes the URL returns a \"false\" string and\n                    // we still want to count those so our Promise\n                    // resolves\n                    index++;\n                    if ( index == ( count - 1 ) ) {\n                        resolve( urls );\n                    }\n                }\n            }, 10 );\n        } );\n    } );\n}\n\ngrabUrls().then( function( urls ) {\n    urls = urls.join( '\\n' );\n    createDownload( urls );\n} );\n\nAfter downloading the urls text file, upload to the jupyter notebook or download the images just as before\n\ndownload_images(dest='./file', urls=Path('/content/urls.txt'))"
  },
  {
    "objectID": "posts/2021-08-25-imagesdownload.html#storing-the-collected-images",
    "href": "posts/2021-08-25-imagesdownload.html#storing-the-collected-images",
    "title": "Images downloading methods",
    "section": "storing the collected images",
    "text": "storing the collected images\nDownloaded files from the colab can be stored using the following code snippet\n\n# collect the files\nzip_name = 'folder_name.zip'\n!rm -f {zip_name}\n!zip -q -r {zip_name} {root}\n\n# download the files to your local computer\nfrom google.colab import files\nfiles.download(zip_name)\n\n# download the files to Google Drive\nfrom google.colab import drive\nimport shutil\ndestination_folder = 'folder_name'\ndrive.mount('/content/drive/')\nfolder = Path('/content/drive/My Drive')/destination_folder\nfolder.mkdir(parents=True, exist_ok = True)\nshutil.copyfile(zip_name, str(folder/zip_name))\n\n# open the collected files\n!unzip \\*zip && rm *.zip\n\n\ncredits\n\nMaria L Rodriguez\njoeDockrill\npyimagesearch"
  },
  {
    "objectID": "posts/2021-08-26-mit_exercise_2.html",
    "href": "posts/2021-08-26-mit_exercise_2.html",
    "title": "Missing semester MIT Exercise 2 Solutions",
    "section": "",
    "text": "MIT Missing semester Lectures are greatly helps one to work with terminal, git -version control systems, data wrangling etc… Here I have listed out solutions for the exercises of each lecture.\n\nRead man ls and write an ls command that lists files in the following manner Includes all files, including hidden files Sizes are listed in human readable format (e.g. 454M instead of 454279954) Files are ordered by recency Output is colorized\n\n\n!ls -laht --color=always\n\ntotal 28K\ndrwxr-xr-x 1 root root 4.0K Aug 26 06:56 .\ndrwxr-xr-x 2 root root 4.0K Aug 26 06:56 bar\ndrwxr-xr-x 2 root root 4.0K Aug 26 06:56 foo\ndrwxr-xr-x 2 root root 4.0K Aug 26 06:56 fooob\ndrwxr-xr-x 1 root root 4.0K Aug 26 06:41 ..\ndrwxr-xr-x 1 root root 4.0K Aug 13 13:35 sample_data\ndrwxr-xr-x 4 root root 4.0K Aug 13 13:34 .config\n\n\n\nWrite bash functions marco and polo that do the following. Whenever you execute marco the current working directory should be saved in some manner, then when you execute polo, no matter what directory you are in, polo should cd you back to the directory where you executed marco. For ease of debugging you can write the code in a file marco.sh and (re)load the definitions to your shell by executing source marco.sh.\n   touch marco.sh polo.sh\n   chmod +x marco.sh polo.sh          \n\n   marco.sh\n   marco () {\n         pwd > marco.txt\n   }\n\n   polo.sh\n   polo () {\n         cd $(cat < marco.txt)\n   }\n\nafter creating marco.sh and polo.sh files in vim call them from anywhere using source command\n        source path/to/workingdir/marco.sh\n\n        source path/to/workingdir/polo.sh\n\nSay you have a command that fails rarely. In order to debug it you need to capture its output but it can be time consuming to get a failure run. Write a bash script that runs the following script until it fails and captures its standard output and error streams to files and prints everything at the end. Bonus points if you can also report how many runs it took for the script to fail."
  },
  {
    "objectID": "about.html#twitter-github-email-linkedin",
    "href": "about.html#twitter-github-email-linkedin",
    "title": "About",
    "section": "Twitter | GitHub | Email | LinkedIn",
    "text": "Twitter | GitHub | Email | LinkedIn\nI am working as Solution Architect in AVEVA (Schneider Electric). I am passionate about data, and deeplearning. I am introduced to deeplearning through fast.ai and have been poisoned by it completely and have started blogging here from then onwards. I hope, one day I will write a great technical paper and publish.\nI am also self-taught - thanks to the wonderful fast.ai course."
  },
  {
    "objectID": "about.html#twitter-github-linkedin",
    "href": "about.html#twitter-github-linkedin",
    "title": "About",
    "section": "Twitter | GitHub | LinkedIn",
    "text": "Twitter | GitHub | LinkedIn\nI am working as Solution Architect in AVEVA (Schneider Electric). I am passionate about data, and deeplearning. I am introduced to deeplearning through fast.ai and have been poisoned by it completely and have started blogging here from then onwards. I hope, one day I will write a great technical paper and publish.\nI am also self-taught - thanks to the wonderful fast.ai course."
  }
]