[
  {
    "objectID": "posts/2021-08-25-imagesdownload.html",
    "href": "posts/2021-08-25-imagesdownload.html",
    "title": "Images downloading methods",
    "section": "",
    "text": "Following are the ways I used to download images from internet for data analysis purpose * Using search_image_ddg * Using Pyimage search method"
  },
  {
    "objectID": "posts/2021-08-25-imagesdownload.html#search_image_ddg-method",
    "href": "posts/2021-08-25-imagesdownload.html#search_image_ddg-method",
    "title": "Images downloading methods",
    "section": "search_image_ddg method",
    "text": "search_image_ddg method\nFrom fastbook webiste you can see the documentaion, however here I reproduce code snippet for ease of use.\n\n!pip install fastbook -Uqq\nimport fastbook\nfastbook.setup_book()\n\nMounted at /content/gdrive\n\n\n\nfrom fastbook import *\n\n\ndef search_images_ddg(term, max_images=200): # search_image_ddg in fastbook version has some issues so defining seperately\n    \"Search for `term` with DuckDuckGo and return a unique urls of about `max_images` images\"\n    assert max_images<1000\n    url = 'https://duckduckgo.com/'\n    res = urlread(url,data={'q':term})\n    searchObj = re.search(r'vqd=([\\d-]+)\\&', res)\n    assert searchObj\n    requestUrl = url + 'i.js'\n    params = dict(l='us-en', o='json', q=term, vqd=searchObj.group(1), f=',,,', p='1', v7exp='a')\n    urls,data = set(),{'next':1}\n    while len(urls)<max_images and 'next' in data:\n        try:\n            data = urljson(requestUrl,data=params)\n            urls.update(L(data['results']).itemgot('image'))\n            requestUrl = url + data['next']\n        except (URLError,HTTPError): pass\n        time.sleep(0.2)\n    return L(urls)\n\n\nresults = search_images_ddg('search word', max_images=300)\n\n\ndownload_images(dest, urls=results) #destination folder path"
  },
  {
    "objectID": "posts/2021-08-25-imagesdownload.html#another-more-elegant-method-using-jmd_imagescraper-thanks-to-joe-dockrill",
    "href": "posts/2021-08-25-imagesdownload.html#another-more-elegant-method-using-jmd_imagescraper-thanks-to-joe-dockrill",
    "title": "Images downloading methods",
    "section": "Another more elegant method using jmd_imagescraper (Thanks to Joe Dockrill)",
    "text": "Another more elegant method using jmd_imagescraper (Thanks to Joe Dockrill)\nRecently I found jmd_imagescraper library from Joe Dockrill. It is lot easier to get images data from web using DuckDuckGo search engine, better than previous method shown.\nFollwoing is the code snippet for downloading any images:\n\n!pip install jmd_imagescraper -Uqq\nfrom jmd_imagescraper.core import *\nfrom pathlib import Path\n\nroot = Path().cwd()/'folder_name'\n\nsearch = duckduckgo_search\n\nsearch(root, 'prez-biden','joe biden ', max_results = 50, img_layout=ImgLayout.All)\n\n\nfrom jmd_imagescraper.imagecleaner import *\ndisplay_image_cleaner(root)\n\n\n\n\n\n\n\n\n\n\nUsing above method we can clean our dataset by deleting whichever is not according to the description of the dataset. using this library we can clearly structure the dataset with proper folder structure."
  },
  {
    "objectID": "posts/2021-08-25-imagesdownload.html#using-pyimage-search-method",
    "href": "posts/2021-08-25-imagesdownload.html#using-pyimage-search-method",
    "title": "Images downloading methods",
    "section": "Using pyimage search method",
    "text": "Using pyimage search method\nPyimage search is well known for computer vision tutorials and Mr Adrian Rosebrock has given code snippet in his blog here\nYou only need to copy the following code snippet into the Javascript console and type enter. Text file will be downloaded comprising all urls related to the searched images. javascript console will be opened in browser by pressing F12 key in either mozilla firefox or chrome.\n\nfunction simulateRightClick( element ) {\n    var event1 = new MouseEvent( 'mousedown', {\n        bubbles: true,\n        cancelable: false,\n        view: window,\n        button: 2,\n        buttons: 2,\n        clientX: element.getBoundingClientRect().x,\n        clientY: element.getBoundingClientRect().y\n    } );\n    element.dispatchEvent( event1 );\n    var event2 = new MouseEvent( 'mouseup', {\n        bubbles: true,\n        cancelable: false,\n        view: window,\n        button: 2,\n        buttons: 0,\n        clientX: element.getBoundingClientRect().x,\n        clientY: element.getBoundingClientRect().y\n    } );\n    element.dispatchEvent( event2 );\n    var event3 = new MouseEvent( 'contextmenu', {\n        bubbles: true,\n        cancelable: false,\n        view: window,\n        button: 2,\n        buttons: 0,\n        clientX: element.getBoundingClientRect().x,\n        clientY: element.getBoundingClientRect().y\n    } );\n    element.dispatchEvent( event3 );\n}\n\nfunction getURLParam( queryString, key ) {\n    var vars = queryString.replace( /^\\?/, '' ).split( '&' );\n    for ( let i = 0; i < vars.length; i++ ) {\n        let pair = vars[ i ].split( '=' );\n        if ( pair[0] == key ) {\n            return pair[1];\n        }\n    }\n    return false;\n}\n\nfunction createDownload( contents ) {\n    var hiddenElement = document.createElement( 'a' );\n    hiddenElement.href = 'data:attachment/text,' + encodeURI( contents );\n    hiddenElement.target = '_blank';\n    hiddenElement.download = 'urls.txt';\n    hiddenElement.click();\n}\n\nfunction grabUrls() {\n    var urls = [];\n    return new Promise( function( resolve, reject ) {\n        var count = document.querySelectorAll(\n            '.isv-r a:first-of-type' ).length,\n            index = 0;\n        Array.prototype.forEach.call( document.querySelectorAll(\n            '.isv-r a:first-of-type' ), function( element ) {\n            // using the right click menu Google will generate the\n            // full-size URL; won't work in Internet Explorer\n            // (http://pyimg.co/byukr)\n            simulateRightClick( element.querySelector( ':scope img' ) );\n            // Wait for it to appear on the <a> element\n            var interval = setInterval( function() {\n                if ( element.href.trim() !== '' ) {\n                    clearInterval( interval );\n                    // extract the full-size version of the image\n                    let googleUrl = element.href.replace( /.*(\\?)/, '$1' ),\n                        fullImageUrl = decodeURIComponent(\n                            getURLParam( googleUrl, 'imgurl' ) );\n                    if ( fullImageUrl !== 'false' ) {\n                        urls.push( fullImageUrl );\n                    }\n                    // sometimes the URL returns a \"false\" string and\n                    // we still want to count those so our Promise\n                    // resolves\n                    index++;\n                    if ( index == ( count - 1 ) ) {\n                        resolve( urls );\n                    }\n                }\n            }, 10 );\n        } );\n    } );\n}\n\ngrabUrls().then( function( urls ) {\n    urls = urls.join( '\\n' );\n    createDownload( urls );\n} );\n\nAfter downloading the urls text file, upload to the jupyter notebook or download the images just as before\n\ndownload_images(dest='./file', urls=Path('/content/urls.txt'))"
  },
  {
    "objectID": "posts/2021-08-25-imagesdownload.html#storing-the-collected-images",
    "href": "posts/2021-08-25-imagesdownload.html#storing-the-collected-images",
    "title": "Images downloading methods",
    "section": "storing the collected images",
    "text": "storing the collected images\nDownloaded files from the colab can be stored using the following code snippet\n\n# collect the files\nzip_name = 'folder_name.zip'\n!rm -f {zip_name}\n!zip -q -r {zip_name} {root}\n\n# download the files to your local computer\nfrom google.colab import files\nfiles.download(zip_name)\n\n# download the files to Google Drive\nfrom google.colab import drive\nimport shutil\ndestination_folder = 'folder_name'\ndrive.mount('/content/drive/')\nfolder = Path('/content/drive/My Drive')/destination_folder\nfolder.mkdir(parents=True, exist_ok = True)\nshutil.copyfile(zip_name, str(folder/zip_name))\n\n# open the collected files\n!unzip \\*zip && rm *.zip\n\n\ncredits\n\nMaria L Rodriguez\njoeDockrill\npyimagesearch"
  },
  {
    "objectID": "posts/leapfaster/2023-04-06-leapfaster.html",
    "href": "posts/leapfaster/2023-04-06-leapfaster.html",
    "title": "Leap faster with ChatGPT",
    "section": "",
    "text": "Its just 4 months old, entire internet is hijacked by the word GPT and all sorts of GPT tools are emerging and filling us with information overlaod. The ramifications on scociety is so huge and can effect the society from the roots. This advancement is unlike the other two major technical advancements like electricity and internet. The words of the foremost AI research scientist “Jeremy Howard” and the popular python library Django founder “Simon Willison”, will set the context of todays blog.\n\n\nI think Simon's advice is still the best approach for now, since I can't think of anything better.But assuming this time is just like all the other times, might lead to hubris.\n\n— Jeremy Howard (@jeremyphoward) March 26, 2023\n\n\n\n\nIf you're just starting to learn software engineering right now but you're considering dropping it because you think the field might be made obsolete by AI, I have an alternative approach to suggest for you:Start learning now, and use AI tools to learn FASTER\n\n— Simon Willison (@simonw) March 25, 2023\n\n\nTimes like this scares many people and generates questions like: will I be replaced with AI, will there be paycuts, will my job get obseleted etc. The rapid development of Large Language Models (LLM) by various reasearch labs definitely pushes one to validate themselves in the new found ecosystems. Quite contrary to the sentiment, it is actually prime time to catapult oneself to epitome of their careers with the new Equalizer in the market."
  },
  {
    "objectID": "posts/leapfaster/2023-04-06-leapfaster.html#equalizer",
    "href": "posts/leapfaster/2023-04-06-leapfaster.html#equalizer",
    "title": "Leap faster with ChatGPT",
    "section": "Equalizer",
    "text": "Equalizer\nChatGPT equalized the disadvantaged, beginners, and amateures alike with senior most developers in organisations across the planet. Suddenly it has given a mentor, advisor, apprentice and intern for various people at their convenience at various levels depends on their level of expertise in their corresponding fields. The benefits you can leverage depends on the skilled prompt you can pose to Generative Pretraining Transformer (GPT) model. There are many GPT models in the market, chatGPT is humongous and powerful in the market.\nI would encourage one to engage with GPT mdoels, whatever the way possible. It can be playing with chatGPT prompts at openAI website, installing various GPT extensions in VScode, tinkering the opensource model cards in huggingface, or just browsing twitter over the developments of GPT models and leveraging them to our advantage. One would be surprised to realise the productivity gains, value additios that one can be profited with. Recently, I witnessed my friend develop a static website from scratch using chatGPT. Despite having no prior knowledge or experience in development, He was able to create website ready for hosting within an hour of interacting with this new technical marvel.\nTwitter open-sourced their recommnedation algorithms few days back and it would be great to understand the internals of the code. Normally I have to depend on the google, stackoverflow and youtube to get to know about some geeky stuff. Now I simply copy the code and ask chatGPT to explain the code. The beauty of that can be seen in the following image.\n\nselect “Explain the selected code” and then you have chatGPT explaining all the code.\n\nTwitter open-sourced algorithm: code snippet\nAnother great jupyter notebook magic functions written by Mr Radek Osmulski can be worth exploring at ask_ai repo in github. Radek is Kaggle Grand-Master and profusely encouraging people to integrate chatGPT into their learning styles. What I understood from radek writings and podcasts on AI is, you no need to be a great programmer, but you need to be great prompter to learn and get things done at faster rate."
  },
  {
    "objectID": "posts/leapfaster/2023-04-06-leapfaster.html#explore-ai",
    "href": "posts/leapfaster/2023-04-06-leapfaster.html#explore-ai",
    "title": "Leap faster with ChatGPT",
    "section": "Explore AI",
    "text": "Explore AI\nChatGPT is just NLP (Natural Language Processing) model, but AI has been into many domains and Midjourney is mindblowingly working with stable diffusion (chatGPT for images) models and one must get involve to get the taste and beauty of AI in image world.\nFollowing are the few people I see worth following in twitter to be abreast of the developments\n\nEmad -Stability AI founder\nSimon Willison -Django founder\nJeremy Howard -fastai founder\nTanishq Mathew Abraham -19y old AI prodigy\nOpenAI\nNick St. Pierre\nLinus\n\nChatGPT has reduced all the barriers between learner and learned. Its all about what one want and how one prompts the engine to your advantage. Having said all of this, it must be taken with pinch of salt. LLMs (Large language models) are so sophisticated that even when they lies its not easy to find and they can configure a lie which looks more truer than truth. But it can be found out with little interest in the subject and inquisitiveness to know the truth. Effort and doubt are two must ingredients to learn with chatGPT with 10X speed. Its not uncommon to find mistakes and unlearning in path to perfection and it is not different with chatGPT. More than anything, now one have personal AI assitant to help you achieve one’s goals at their leisure.\nJust to give you the taste of the rapidity with which AI is going to transform our lives, just look at this reddit thread. Both fear and awe will stuck you at same time."
  },
  {
    "objectID": "posts/2021-08-26-mit_exercise_2.html",
    "href": "posts/2021-08-26-mit_exercise_2.html",
    "title": "Missing semester MIT Exercise 2 Solutions",
    "section": "",
    "text": "MIT Missing semester Lectures are greatly helps one to work with terminal, git -version control systems, data wrangling etc… Here I have listed out solutions for the exercises of each lecture.\n\nRead man ls and write an ls command that lists files in the following manner Includes all files, including hidden files Sizes are listed in human readable format (e.g. 454M instead of 454279954) Files are ordered by recency Output is colorized\n\n\n!ls -laht --color=always\n\ntotal 28K\ndrwxr-xr-x 1 root root 4.0K Aug 26 06:56 .\ndrwxr-xr-x 2 root root 4.0K Aug 26 06:56 bar\ndrwxr-xr-x 2 root root 4.0K Aug 26 06:56 foo\ndrwxr-xr-x 2 root root 4.0K Aug 26 06:56 fooob\ndrwxr-xr-x 1 root root 4.0K Aug 26 06:41 ..\ndrwxr-xr-x 1 root root 4.0K Aug 13 13:35 sample_data\ndrwxr-xr-x 4 root root 4.0K Aug 13 13:34 .config\n\n\n\nWrite bash functions marco and polo that do the following. Whenever you execute marco the current working directory should be saved in some manner, then when you execute polo, no matter what directory you are in, polo should cd you back to the directory where you executed marco. For ease of debugging you can write the code in a file marco.sh and (re)load the definitions to your shell by executing source marco.sh.\n   touch marco.sh polo.sh\n   chmod +x marco.sh polo.sh          \n\n   marco.sh\n   marco () {\n         pwd > marco.txt\n   }\n\n   polo.sh\n   polo () {\n         cd $(cat < marco.txt)\n   }\n\nafter creating marco.sh and polo.sh files in vim call them from anywhere using source command\n        source path/to/workingdir/marco.sh\n\n        source path/to/workingdir/polo.sh\n\nSay you have a command that fails rarely. In order to debug it you need to capture its output but it can be time consuming to get a failure run. Write a bash script that runs the following script until it fails and captures its standard output and error streams to files and prints everything at the end. Bonus points if you can also report how many runs it took for the script to fail."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Weblogs",
    "section": "",
    "text": "I am deeply passionate about deep learning. I have started my journey into ML/AI space in 2020 and have been overwhelmed and fascinated by its sheer curiosity generating concepts. This blog is a way to store and publish my understandings and if it helps anybody in anyway that would be great. \n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nLeap faster with ChatGPT\n\n\n\n\n\n\n\nAGI\n\n\n\n\nChatGPT: Personal AI assistant\n\n\n\n\n\n\nApr 8, 2023\n\n\n\n\n\n\n  \n\n\n\n\nFastai fit_one_cycle & fine_tune and Super-Convergence (Leslie Smith)\n\n\n\n\n\n\n\npaper_reading\n\n\n\n\nExploring Source code of fastai\n\n\n\n\n\n\nOct 14, 2021\n\n\n\n\n\n\n  \n\n\n\n\nConvolutional Neural Networks. Fastbook chapter 13\n\n\n\n\n\n\n\nfastbook\n\n\n\n\nVisualising CNN\n\n\n\n\n\n\nSep 20, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMissing semester MIT Exercise 2 Solutions\n\n\n\n\n\n\n\n\n\n\n\n\nAug 26, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImages downloading methods\n\n\n\n\n\n\n\n\n\n\n\n\nAug 25, 2021\n\n\n\n\n\n\n  \n\n\n\n\nModi’s Beard. Fastai chapter 2\n\n\n\n\n\n\n\nfastbook\n\n\n\n\nWant to play with Modi’s beard…\n\n\n\n\n\n\nAug 4, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "about.html#twitter-github-linkedin",
    "href": "about.html#twitter-github-linkedin",
    "title": "About",
    "section": "Twitter | GitHub | LinkedIn",
    "text": "Twitter | GitHub | LinkedIn\nMy journey into the technology world began quite late in my life. As a mechanical engineer, I never imagined that I would venture into a domain that was not mine. However, the allure of coding, cloud computing, and artificial intelligence eventually beckoned me towards this industry. Initially, I faced resistance and inadequacy because of my non-technical background, but I was determined to make a mark in this fast-evolving technological world.\nThrough self-education and endless hours of hard work, I began to acquire knowledge and skills in coding and data engineering. The internet and online tutorials helped me learn the basics of programming languages such as Python, SQL, and JavaScript. Slowly but surely, I gained more proficiency and started working on small projects to strengthen my skills further. And now, I find myself drawn to the fields of machine learning, artificial intelligence, and cloud DevOps, and excited to delve deeper into these areas.\nThis blog is an attempt to share my journey and experiences as a self-taught programmer in a world that is transitioning rapidly towards technology. Here, I hope to share my knowledge and provide insights into the fascinating world of computer engineering, data analysis, and software development. It is my hope that my story will inspire others to embrace their passions and pursue their dreams, regardless of their initial educational background.\nI am presently working as Solution Architect in AVEVA (Schneider Electric). I am passionate about data, and deeplearning. I am introduced to deeplearning through fast.ai and have been poisoned by it completely and have started blogging here from then onwards. I hope, one day I will write a great technical paper and publish.\nI am also self-taught - thanks to the wonderful fast.ai course."
  },
  {
    "objectID": "LeapFaster.html",
    "href": "LeapFaster.html",
    "title": "mldurga",
    "section": "",
    "text": "Leap faster with ChatGPT\n\ntoc: true\nbranch: master\nbadges: true\ncomments: true\ncategories: [AGI]\nimage: https://user-images.githubusercontent.com/19243618/137338003-dfdbf632-4ea7-444b-af40-e68319d076d8.png\nhide: false\nsearch_exclude: true\nmetadata_key1: metadata_value1\nmetadata_key2: metadata_value2\n\n\nChatGPT - Personal AI assistant\n\nIts just 4 months old, entire internet is hijacked by the word GPT and all sorts of GPT tools are emerging and filling us with information overlaod. The ramifications on scociety is so huge and can effect the society from the roots. This advancement is unlike the other two major technical advancements electricity and internet. In the words of the foremost AI research scientist “Jeremy Howard” and the popular python library Django founder “Simon Wilson”, will set the context of todays blog.\n\n\nI think Simon's advice is still the best approach for now, since I can't think of anything better.But assuming this time is just like all the other times, might lead to hubris.\n\n— Jeremy Howard (@jeremyphoward) March 26, 2023\n\n\n\n\nIf you're just starting to learn software engineering right now but you're considering dropping it because you think the field might be made obsolete by AI, I have an alternative approach to suggest for you:Start learning now, and use AI tools to learn FASTER\n\n— Simon Willison (@simonw) March 25, 2023"
  }
]